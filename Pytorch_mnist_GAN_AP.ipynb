{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Ci7mB6vuYNr"
      },
      "outputs": [],
      "source": [
        "# modified version of notebook from https://github.com/lyeoni/pytorch-mnist-GAN\n",
        "\n",
        "\n",
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import copy\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "batch_size = 100\n",
        "z_dim = 100\n",
        "learning_rate = 0.0002\n",
        "n_epoch = 10  # Training epochs for each GAN\n",
        "num_tasks = 5  # Number of class-incremental tasks\n",
        "mnist_dim = 28 * 28  # Dimension of the MNIST data"
      ],
      "metadata": {
        "id": "XCkXNvHFFTss"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2uZMgCEvuYNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992e8be9-ba98-4545-f3ac-0ed3322c4629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 12567574.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 368529.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3139947.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3253164.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
        "\n",
        "# Split dataset into five tasks, each containing two classes\n",
        "def split_dataset(dataset, num_tasks=5):\n",
        "    task_indices = []\n",
        "    targets = np.array(dataset.targets)\n",
        "    classes = np.unique(targets)\n",
        "    for i in range(num_tasks):\n",
        "        idx = np.where(np.isin(targets, classes[i*2:i*2+2]))[0]\n",
        "        task_indices.append(idx)\n",
        "    return [Subset(dataset, idx) for idx in task_indices]\n",
        "\n",
        "train_tasks = split_dataset(train_dataset, num_tasks)\n",
        "test_tasks = split_dataset(test_dataset, num_tasks)\n",
        "\n",
        "train_loaders = [DataLoader(task, batch_size=batch_size, shuffle=True) for task in train_tasks]\n",
        "test_loaders = [DataLoader(task, batch_size=batch_size, shuffle=False) for task in test_tasks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7LDGqfexuYNs"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train GAN (Generator and Discriminator)\n",
        "def train_gan(generator, discriminator, data_loader, epochs=5):\n",
        "    criterion = nn.BCELoss()\n",
        "    G_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    D_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for data, _ in data_loader:\n",
        "            batch_size = data.size(0)\n",
        "            data = data.view(batch_size, -1).to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            D_optimizer.zero_grad()\n",
        "            outputs = discriminator(data)\n",
        "            d_loss_real = criterion(outputs, real_labels)\n",
        "            d_loss_real.backward()\n",
        "\n",
        "            z = torch.randn(batch_size, z_dim).to(device)\n",
        "            fake_data = generator(z)\n",
        "            outputs = discriminator(fake_data.detach())\n",
        "            d_loss_fake = criterion(outputs, fake_labels)\n",
        "            d_loss_fake.backward()\n",
        "            D_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            G_optimizer.zero_grad()\n",
        "            outputs = discriminator(fake_data)\n",
        "            g_loss = criterion(outputs, real_labels)\n",
        "            g_loss.backward()\n",
        "            G_optimizer.step()"
      ],
      "metadata": {
        "id": "loVUNUiEsfDB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Classifier model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim=784, num_classes=10):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "i-zcVm3psweh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the Classifier\n",
        "def train_classifier(model, optimizer, criterion, train_loader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "ZAE9r7d_szrO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function to calculate accuracy\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in data_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "WGr1NsmFs6ei"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Incremental Learning function\n",
        "def class_incremental_learning(train_loaders, test_loaders, num_tasks=5):\n",
        "    classifier = Classifier(num_classes=10).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "    all_acc = []\n",
        "    avg_acc = []\n",
        "\n",
        "    for i in range(num_tasks):\n",
        "        print(f\"\\nTraining on task {i+1}/{num_tasks}...\")\n",
        "\n",
        "        # Train GAN for the current task\n",
        "        generator = Generator(g_input_dim=z_dim, g_output_dim=mnist_dim).to(device)\n",
        "        discriminator = Discriminator(d_input_dim=mnist_dim).to(device)\n",
        "        train_gan(generator, discriminator, train_loaders[i], epochs=n_epoch)\n",
        "\n",
        "        # Generate synthetic data for previous tasks\n",
        "        for j in range(i):\n",
        "            num_samples = len(train_tasks[j])\n",
        "            z = torch.randn(num_samples, z_dim).to(device)\n",
        "            synthetic_data = generator(z).view(num_samples, 1, 28, 28).detach().cpu()\n",
        "            synthetic_targets = torch.tensor([j for _ in range(num_samples)])\n",
        "            synthetic_dataset = torch.utils.data.TensorDataset(synthetic_data, synthetic_targets)\n",
        "            synthetic_loader = DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            # Train classifier on synthetic data\n",
        "            train_classifier(classifier, optimizer, criterion, synthetic_loader)\n",
        "\n",
        "        # Train classifier on new task's real data\n",
        "        train_classifier(classifier, optimizer, criterion, train_loaders[i])\n",
        "\n",
        "        # Evaluate on all seen tasks\n",
        "        acc_per_task = []\n",
        "        for j in range(i+1):\n",
        "            acc = evaluate(classifier, test_loaders[j])\n",
        "            acc_per_task.append(acc)\n",
        "            print(f\"Accuracy on task {j+1}: {acc:.2f}%\")\n",
        "\n",
        "        avg_acc.append(np.mean(acc_per_task))\n",
        "        all_acc.append(acc_per_task)\n",
        "\n",
        "    return all_acc, avg_acc"
      ],
      "metadata": {
        "id": "SkE9MVids-M8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the class-incremental learning\n",
        "all_acc, avg_acc = class_incremental_learning(train_loaders, test_loaders)\n",
        "\n",
        "print(\"\\nFinal Average Accuracy per Task:\")\n",
        "for i, acc in enumerate(avg_acc):\n",
        "    print(f\"Task {i+1}: {acc:.2f}%\")\n",
        "\n",
        "# Compute Forward Transfer and Backward Transfer\n",
        "fwt = (avg_acc[-1] - avg_acc[0]) / avg_acc[0] * 100\n",
        "bwt = np.mean([avg_acc[-1] - acc for acc in avg_acc[:-1]])\n",
        "\n",
        "print(f\"\\nForward Transfer: {fwt:.2f}%\")\n",
        "print(f\"Backward Transfer: {bwt:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Ea7GMQtgya",
        "outputId": "b9939f0f-2fa8-43dc-a88a-ea92f2ab77ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on task 1/5...\n",
            "Accuracy on task 1: 99.91%\n",
            "\n",
            "Training on task 2/5...\n",
            "Accuracy on task 1: 0.00%\n",
            "Accuracy on task 2: 97.89%\n",
            "\n",
            "Training on task 3/5...\n",
            "Accuracy on task 1: 0.00%\n",
            "Accuracy on task 2: 0.00%\n",
            "Accuracy on task 3: 99.36%\n",
            "\n",
            "Training on task 4/5...\n",
            "Accuracy on task 1: 0.00%\n",
            "Accuracy on task 2: 0.00%\n",
            "Accuracy on task 3: 0.00%\n",
            "Accuracy on task 4: 99.65%\n",
            "\n",
            "Training on task 5/5...\n",
            "Accuracy on task 1: 0.00%\n",
            "Accuracy on task 2: 0.00%\n",
            "Accuracy on task 3: 0.00%\n",
            "Accuracy on task 4: 0.00%\n",
            "Accuracy on task 5: 98.08%\n",
            "\n",
            "Final Average Accuracy per Task:\n",
            "Task 1: 99.91%\n",
            "Task 2: 48.95%\n",
            "Task 3: 33.12%\n",
            "Task 4: 24.91%\n",
            "Task 5: 19.62%\n",
            "\n",
            "Forward Transfer: -80.36%\n",
            "Backward Transfer: -32.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set example dimensions\n",
        "g_input_dim = 100  # Example input dimension for Generator\n",
        "g_output_dim = 784  # Example output dimension for Generator\n",
        "\n",
        "input_dim = 784  # Example input dimension for Classifier\n",
        "output_dim = 10  # Example output dimension for Classifier\n",
        "\n",
        "# Create an instance of the Generator and Classifier\n",
        "generator = Generator(g_input_dim, g_output_dim)\n",
        "classifier = Classifier(input_dim, output_dim)\n",
        "\n",
        "# 1. Measure the memory and (estimate) computational requirements for generative and vanilla replay with raw samples\n",
        "gan_memory = sum(p.numel() for p in generator.parameters())\n",
        "cla_memory = sum(p.numel() for p in classifier.parameters())\n",
        "\n",
        "# Print memory requirements\n",
        "print(f\"Memory requirements:\")\n",
        "print(f\"Generator: {gan_memory:.2f}\")\n",
        "print(f\"Classifier: {cla_memory:.2f}\")\n",
        "print()\n",
        "\n",
        "# Estimate computational requirements (FLOPS)\n",
        "# This is a simplified estimate and may vary based on actual operations and hardware\n",
        "# Assume input size for estimation\n",
        "# Example input sizes\n",
        "input_size_generator = (1, g_input_dim)  # Example input size for Generator\n",
        "input_size_classifier = (1, input_dim)    # Example input size for Classifier\n",
        "\n",
        "# Estimate computational requirements (FLOPS) - This is a simplified estimation\n",
        "# Generator FLOPS estimation\n",
        "dummy_input_generator = torch.randn(input_size_generator)\n",
        "num_operations_generator = g_input_dim * g_output_dim * 2  # Assume simple linear operation\n",
        "generator_flops = num_operations_generator\n",
        "\n",
        "# Classifier FLOPS estimation\n",
        "dummy_input_classifier = torch.randn(input_size_classifier)\n",
        "num_operations_classifier = input_dim * output_dim * 2  # Assume simple linear operation\n",
        "classifier_flops = num_operations_classifier\n",
        "\n",
        "# Print computational requirements\n",
        "print(f\"Computational requirements (FLOPS):\")\n",
        "print(f\"Generator: {generator_flops} FLOPS\")\n",
        "print(f\"Classifier: {classifier_flops} FLOPS\")\n",
        "print()\n",
        "\n",
        "# Comment on memory requirements comparison\n",
        "if gan_memory == cla_memory:\n",
        "    print(\"Memory requirements are comparable.\")\n",
        "else:\n",
        "    print(\"Memory requirements are not comparable due to differences in parameter counts.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EynAKsa92jDo",
        "outputId": "468df148-03a1-493d-94b7-572cb35e902a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory requirements:\n",
            "Generator: 1486352.00\n",
            "Classifier: 235146.00\n",
            "\n",
            "Computational requirements (FLOPS):\n",
            "Generator: 156800 FLOPS\n",
            "Classifier: 15680 FLOPS\n",
            "\n",
            "Memory requirements are not comparable due to differences in parameter counts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize cumulative memory usage\n",
        "cumulative_memory = 0\n",
        "\n",
        "# Loop through each split (five splits of two classes each)\n",
        "for split in range(5):\n",
        "    # Calculate memory requirements for Generator and Classifier for this split\n",
        "    generator = Generator(g_input_dim, g_output_dim)\n",
        "    classifier = Classifier(input_dim, output_dim)\n",
        "\n",
        "    # Calculate memory for this split\n",
        "    generator_memory = sum(p.numel() for p in generator.parameters())\n",
        "    classifier_memory = sum(p.numel() for p in classifier.parameters())\n",
        "    split_memory = generator_memory + classifier_memory\n",
        "\n",
        "    # Accumulate cumulative memory usage\n",
        "    cumulative_memory += split_memory\n",
        "\n",
        "    # Print memory usage for this split\n",
        "    print(f\"Memory usage for Split {split+1}: Generator = {generator_memory} parameters, Classifier = {classifier_memory} parameters\")\n",
        "\n",
        "# Print cumulative memory usage\n",
        "print(f\"\\nCumulative memory usage over 5 splits: {cumulative_memory} parameters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQHuuMayRczv",
        "outputId": "03ea429c-2344-4437-f8f1-a59f94ceafb9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage for Split 1: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for Split 2: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for Split 3: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for Split 4: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for Split 5: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "\n",
            "Cumulative memory usage over 5 splits: 8607490 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NEIvjwwiuYNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7f3f97-5538-440b-91c9-c21ac42e7e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tasks: 1\n",
            "Memory usage for Task 2: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Number of tasks: 5\n",
            "Memory usage for Task 6: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Number of tasks: 10\n",
            "Memory usage for Task 11: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Number of tasks: 20\n",
            "Memory usage for Task 21: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Number of tasks: 50\n",
            "Memory usage for Task 51: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Number of tasks: 100\n",
            "Memory usage for Task 101: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "\n",
            "Cumulative memory usage over 5 tasks: 20657976 parameters\n"
          ]
        }
      ],
      "source": [
        "# 2. How would the solution scale with n tasks? Is it linear, quadratic, or something else?\n",
        "# Varying number of tasks (n)\n",
        "n_tasks = [1, 5, 10, 20, 50, 100]  # Example values of n\n",
        "\n",
        "# Function to calculate memory requirements for a given number of tasks (splits)\n",
        "def calculate_memory_usage(n_tasks, g_input_dim, g_output_dim, input_dim, output_dim):\n",
        "    cumulative_memory = 0\n",
        "\n",
        "for n in n_tasks:\n",
        "    print(f\"Number of tasks: {n}\")\n",
        "\n",
        "    # Create instances of Generator and Classifier for each task\n",
        "    generator = Generator(g_input_dim, g_output_dim)\n",
        "    classifier = Classifier(input_dim, output_dim)\n",
        "\n",
        "    # Calculate memory for this task\n",
        "    gan_memory = sum(p.numel() for p in generator.parameters())\n",
        "    cla_memory = sum(p.numel() for p in classifier.parameters())\n",
        "    task_memory = generator_memory + classifier_memory\n",
        "\n",
        "    # Accumulate cumulative memory usage\n",
        "    cumulative_memory += task_memory\n",
        "\n",
        "    # Print memory usage for this task\n",
        "    print(f\"Memory usage for Task {n+1}: Generator = {generator_memory} parameters, Classifier = {classifier_memory} parameters\")\n",
        "\n",
        "# Print cumulative memory usage\n",
        "print(f\"\\nCumulative memory usage over {num_tasks} tasks: {cumulative_memory} parameters\")\n",
        "\n",
        "# Example dimensions and number of tasks\n",
        "g_input_dim = 100  # Example input dimension for Generator\n",
        "g_output_dim = 784  # Example output dimension for Generator\n",
        "input_dim = 784  # Example input dimension for Classifier\n",
        "output_dim = 10  # Example output dimension for Classifier\n",
        "num_tasks = 5  # Number of tasks (splits), each split adds two new classes\n",
        "\n",
        "# Calculate and print memory usage for the specified number of tasks\n",
        "calculate_memory_usage(n_tasks, g_input_dim, g_output_dim, input_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What are the downsides of this approach?\n",
        "# Function to simulate memory usage across multiple tasks (splits)\n",
        "def simulate_memory_usage(num_tasks, g_input_dim, g_output_dim, input_dim, output_dim):\n",
        "    cumulative_memory = 0\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        # Example: Creating Generator and Classifier for each task\n",
        "        generator = Generator(g_input_dim, g_output_dim)\n",
        "        classifier = Classifier(input_dim, output_dim)\n",
        "\n",
        "        # Measure memory usage of generator and classifier\n",
        "        generator_memory = calculate_model_memory(generator)\n",
        "        classifier_memory = calculate_model_memory(classifier)\n",
        "\n",
        "        # Calculate total memory usage for this task\n",
        "        task_memory = generator_memory + classifier_memory\n",
        "\n",
        "        # Accumulate cumulative memory usage\n",
        "        cumulative_memory += task_memory\n",
        "\n",
        "        # Print memory usage for this task\n",
        "        print(f\"Task {task+1}: Generator Memory = {generator_memory} bytes, Classifier Memory = {classifier_memory} bytes\")\n",
        "\n",
        "    # Print cumulative memory usage\n",
        "    print(f\"\\nCumulative Memory Usage over {num_tasks} tasks: {cumulative_memory} bytes\")\n",
        "\n",
        "# Function to calculate memory usage of a model\n",
        "def calculate_model_memory(model):\n",
        "    # Calculate total number of parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Assume each parameter is stored as a float32 (4 bytes)\n",
        "    memory_bytes = total_params * 4\n",
        "\n",
        "    return memory_bytes\n",
        "\n",
        "# Example dimensions and number of tasks\n",
        "g_input_dim = 100  # Example input dimension for Generator\n",
        "g_output_dim = 784  # Example output dimension for Generator\n",
        "input_dim = 784  # Example input dimension for Classifier\n",
        "output_dim = 10  # Example output dimension for Classifier\n",
        "num_tasks = 5  # Number of tasks (splits), each split adds two new classes\n",
        "\n",
        "# Simulate memory usage across the specified number of tasks\n",
        "simulate_memory_usage(num_tasks, g_input_dim, g_output_dim, input_dim, output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOZJEFlUVQ_9",
        "outputId": "663ac5a8-9161-4081-ce7b-7b9d9791fd21"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Generator Memory = 5945408 bytes, Classifier Memory = 940584 bytes\n",
            "Task 2: Generator Memory = 5945408 bytes, Classifier Memory = 940584 bytes\n",
            "Task 3: Generator Memory = 5945408 bytes, Classifier Memory = 940584 bytes\n",
            "Task 4: Generator Memory = 5945408 bytes, Classifier Memory = 940584 bytes\n",
            "Task 5: Generator Memory = 5945408 bytes, Classifier Memory = 940584 bytes\n",
            "\n",
            "Cumulative Memory Usage over 5 tasks: 34429960 bytes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}