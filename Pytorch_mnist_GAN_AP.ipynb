{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# modified version of notebook from https://github.com/lyeoni/pytorch-mnist-GAN\n",
        "# CL_course-phd-data-science assignment solution code by student: Angela Pomaro\n",
        "\n",
        "\n",
        "# Prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import copy\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 100\n",
        "z_dim = 100\n",
        "learning_rate = 0.0002\n",
        "n_epoch = 10  # Training epochs for each GAN\n",
        "num_tasks = 5  # Number of class-incremental tasks\n",
        "mnist_dim = 28 * 28  # Dimension of the MNIST data\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
        "\n",
        "# Split dataset into five tasks, each containing two classes\n",
        "def split_dataset(dataset, num_tasks=5):\n",
        "    task_indices = []\n",
        "    targets = np.array(dataset.targets)\n",
        "    classes = np.unique(targets)\n",
        "    for i in range(num_tasks):\n",
        "        idx = np.where(np.isin(targets, classes[i*2:i*2+2]))[0]\n",
        "        task_indices.append(idx)\n",
        "    return [Subset(dataset, idx) for idx in task_indices]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features // 2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features // 2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))\n",
        "\n",
        "def train_gan(generator, discriminator, data_loader, epochs=5):\n",
        "    criterion = nn.BCELoss()\n",
        "    G_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    D_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for data, _ in data_loader:\n",
        "            batch_size = data.size(0)\n",
        "            data = data.view(batch_size, -1).to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            D_optimizer.zero_grad()\n",
        "            outputs = discriminator(data)\n",
        "            d_loss_real = criterion(outputs, real_labels)\n",
        "            d_loss_real.backward()\n",
        "\n",
        "            z = torch.randn(batch_size, z_dim).to(device)\n",
        "            fake_data = generator(z)\n",
        "            outputs = discriminator(fake_data.detach())\n",
        "            d_loss_fake = criterion(outputs, fake_labels)\n",
        "            d_loss_fake.backward()\n",
        "            D_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            G_optimizer.zero_grad()\n",
        "            outputs = discriminator(fake_data)\n",
        "            g_loss = criterion(outputs, real_labels)\n",
        "            g_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim=784, num_classes=10):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "def train_classifier(model, optimizer, criterion, train_loader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in data_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def class_incremental_learning(train_dataset, test_dataset, num_tasks=5):\n",
        "    classifier = Classifier(num_classes=10).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "    all_acc = []\n",
        "    avg_acc = []\n",
        "\n",
        "    train_tasks = split_dataset(train_dataset, num_tasks)\n",
        "    test_tasks = split_dataset(test_dataset, num_tasks)\n",
        "\n",
        "    train_loaders = [DataLoader(task, batch_size=batch_size, shuffle=True) for task in train_tasks]\n",
        "    test_loaders = [DataLoader(task, batch_size=batch_size, shuffle=False) for task in test_tasks]\n",
        "\n",
        "    stored_generators = []\n",
        "    synthetic_datasets = []\n",
        "\n",
        "    for i in range(num_tasks):\n",
        "        print(f\"\\nTraining on task {i+1}/{num_tasks}...\")\n",
        "\n",
        "        # Train GAN for the current task\n",
        "        generator = Generator(g_input_dim=z_dim, g_output_dim=mnist_dim).to(device)\n",
        "        discriminator = Discriminator(d_input_dim=mnist_dim).to(device)\n",
        "        train_gan(generator, discriminator, train_loaders[i], epochs=n_epoch)\n",
        "\n",
        "        # Store the current generator\n",
        "        stored_generators.append(copy.deepcopy(generator))\n",
        "\n",
        "        # Generate synthetic data for previous tasks\n",
        "        if i > 0:\n",
        "            for j in range(i):\n",
        "                num_samples = len(train_tasks[j])\n",
        "                z = torch.randn(num_samples, z_dim).to(device)\n",
        "                synthetic_data = stored_generators[j](z).view(num_samples, 1, 28, 28).detach().cpu()\n",
        "                synthetic_targets = torch.tensor([train_tasks[j][k][1] for k in range(num_samples)])\n",
        "                synthetic_dataset = torch.utils.data.TensorDataset(synthetic_data, synthetic_targets)\n",
        "                synthetic_datasets.append(synthetic_dataset)\n",
        "\n",
        "        # Combine real data from current task with synthetic data from previous tasks\n",
        "        combined_data = []\n",
        "        combined_targets = []\n",
        "\n",
        "        # Add real data from current task\n",
        "        for data, targets in train_loaders[i]:\n",
        "            combined_data.append(data)\n",
        "            combined_targets.append(targets)\n",
        "\n",
        "        # Add synthetic data from previous tasks\n",
        "        for synthetic_dataset in synthetic_datasets:\n",
        "            for data, targets in DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True):\n",
        "                combined_data.append(data)\n",
        "                combined_targets.append(targets)\n",
        "\n",
        "        combined_data = torch.cat(combined_data)\n",
        "        combined_targets = torch.cat(combined_targets)\n",
        "        combined_dataset = torch.utils.data.TensorDataset(combined_data, combined_targets)\n",
        "        combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Train classifier on combined data\n",
        "        train_classifier(classifier, optimizer, criterion, combined_loader, epochs=n_epoch)\n",
        "\n",
        "        # Evaluate on all seen tasks\n",
        "        acc_per_task = []\n",
        "        for j in range(i+1):\n",
        "            acc = evaluate(classifier, test_loaders[j])\n",
        "            acc_per_task.append(acc)\n",
        "            print(f\"Accuracy on task {j+1}: {acc:.2f}%\")\n",
        "\n",
        "        avg_acc.append(np.mean(acc_per_task))\n",
        "        all_acc.append(acc_per_task)\n",
        "\n",
        "    return all_acc, avg_acc, classifier\n",
        "\n",
        "# Run the class-incremental learning\n",
        "all_acc, avg_acc, classifier = class_incremental_learning(train_dataset, test_dataset)\n",
        "\n",
        "print(\"\\nFinal Average Accuracy per Task:\")\n",
        "for i, acc in enumerate(avg_acc):\n",
        "    print(f\"Task {i+1}: {acc:.2f}%\")\n",
        "\n",
        "# Compute Forward Transfer and Backward Transfer\n",
        "fwt = (avg_acc[-1] - avg_acc[0]) / avg_acc[0] * 100\n",
        "bwt = np.mean([avg_acc[-1] - acc for acc in avg_acc[:-1]])\n",
        "\n",
        "print(f\"\\nForward Transfer: {fwt:.2f}%\")\n",
        "print(f\"Backward Transfer: {bwt:.2f}%\")\n",
        "print()\n",
        "print()\n",
        "\n",
        "# Memory and computational requirements\n",
        "generator = Generator(g_input_dim=z_dim, g_output_dim=mnist_dim)\n",
        "classifier = Classifier(input_dim=mnist_dim, num_classes=10)\n",
        "\n",
        "gan_memory = sum(p.numel() for p in generator.parameters())\n",
        "cla_memory = sum(p.numel() for p in classifier.parameters())\n",
        "\n",
        "# Print memory requirements\n",
        "print(f\"Memory requirements:\")\n",
        "print(f\"Generator: {gan_memory} parameters\")\n",
        "print(f\"Classifier: {cla_memory} parameters\")\n",
        "print()\n",
        "\n",
        "# Estimate computational requirements (FLOPS)\n",
        "num_operations_generator = z_dim * mnist_dim * 2  # Assume simple linear operation\n",
        "generator_flops = num_operations_generator\n",
        "\n",
        "# Classifier FLOPS estimation\n",
        "num_operations_classifier = mnist_dim * 10 * 2  # Assume simple linear operation\n",
        "classifier_flops = num_operations_classifier\n",
        "\n",
        "# Print computational requirements\n",
        "print(f\"Computational requirements (FLOPS):\")\n",
        "print(f\"Generator: {generator_flops} FLOPS\")\n",
        "print(f\"Classifier: {classifier_flops} FLOPS\")\n",
        "print()\n",
        "\n",
        "# Cumulative memory usage over 5 tasks\n",
        "\n",
        "# Initialize cumulative memory usage\n",
        "cumulative_memory = 0\n",
        "\n",
        "# Loop through each task (of two classes each)\n",
        "for task in range(5):\n",
        "    # Calculate memory requirements for Generator and Classifier for this task\n",
        "    generator = Generator(g_input_dim=z_dim, g_output_dim=mnist_dim)\n",
        "    classifier = Classifier(input_dim=mnist_dim, num_classes=10)\n",
        "\n",
        "    # Calculate memory for this task\n",
        "    generator_memory = sum(p.numel() for p in generator.parameters())\n",
        "    classifier_memory = sum(p.numel() for p in classifier.parameters())\n",
        "    task_memory = generator_memory + classifier_memory\n",
        "\n",
        "    # Accumulate cumulative memory usage\n",
        "    cumulative_memory += task_memory\n",
        "\n",
        "    # Print memory usage for this task\n",
        "    print(f\"Memory usage for task {task+1}: Generator = {generator_memory} parameters, Classifier = {classifier_memory} parameters\")\n",
        "\n",
        "# Print cumulative memory usage\n",
        "print(f\"\\nCumulative memory usage over 5 tasks: {cumulative_memory} parameters\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl7zWRF_MNZE",
        "outputId": "f31b1d53-06c2-4ca7-c867-e2f4476431d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on task 1/5...\n",
            "Accuracy on task 1: 99.95%\n",
            "\n",
            "Training on task 2/5...\n",
            "Accuracy on task 1: 7.14%\n",
            "Accuracy on task 2: 99.66%\n",
            "\n",
            "Training on task 3/5...\n",
            "Accuracy on task 1: 0.14%\n",
            "Accuracy on task 2: 22.92%\n",
            "Accuracy on task 3: 99.89%\n",
            "\n",
            "Training on task 4/5...\n",
            "Accuracy on task 1: 0.28%\n",
            "Accuracy on task 2: 15.52%\n",
            "Accuracy on task 3: 23.69%\n",
            "Accuracy on task 4: 99.85%\n",
            "\n",
            "Training on task 5/5...\n",
            "Accuracy on task 1: 0.00%\n",
            "Accuracy on task 2: 8.77%\n",
            "Accuracy on task 3: 0.05%\n",
            "Accuracy on task 4: 2.72%\n",
            "Accuracy on task 5: 99.34%\n",
            "\n",
            "Final Average Accuracy per Task:\n",
            "Task 1: 99.95%\n",
            "Task 2: 53.40%\n",
            "Task 3: 40.98%\n",
            "Task 4: 34.84%\n",
            "Task 5: 22.18%\n",
            "\n",
            "Forward Transfer: -77.81%\n",
            "Backward Transfer: -35.12%\n",
            "\n",
            "\n",
            "Memory requirements:\n",
            "Generator: 1486352 parameters\n",
            "Classifier: 235146 parameters\n",
            "\n",
            "Computational requirements (FLOPS):\n",
            "Generator: 156800 FLOPS\n",
            "Classifier: 15680 FLOPS\n",
            "\n",
            "Memory usage for task 1: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for task 2: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for task 3: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for task 4: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "Memory usage for task 5: Generator = 1486352 parameters, Classifier = 235146 parameters\n",
            "\n",
            "Cumulative memory usage over 5 tasks: 8607490 parameters\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}